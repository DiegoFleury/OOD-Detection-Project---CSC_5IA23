{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15beea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OOD Detection Notebook Template\n",
    "Convert to .ipynb using: jupytext --to notebook 02_ood_detection.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bf9d1",
   "metadata": {},
   "source": [
    "# Q2: Out-of-Distribution Detection\n",
    "\n",
    "Evaluate 6 OOD scoring methods across training epochs:\n",
    "- Output-based: MSP, MaxLogit, Energy\n",
    "- Distance-based: Mahalanobis  \n",
    "- Feature-based: ViM, NECO (TPT only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e20614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if not os.path.exists('/content/OOD-Detection-Project---CSC_5IA23'):\n",
    "    !git clone https://github.com/DiegoFleury/OOD-Detection-Project---CSC_5IA23.git\n",
    "%cd /content/OOD-Detection-Project---CSC_5IA23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d3a87",
   "metadata": {
    "title": "Setup"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models import ResNet18\n",
    "from src.data import get_cifar100_loaders, get_ood_loaders\n",
    "from src.ood_scores import (\n",
    "    MSPScorer, MaxLogitScorer, EnergyScorer,\n",
    "    MahalanobisScorer, ViMScorer, NECOScorer\n",
    ")\n",
    "from src.utils.ood_metrics import compute_auroc, compute_fpr_at_tpr\n",
    "from src.utils.visualization import plot_ood_scores\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(yaml.dump(config,default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0404ee8",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Load Data"
   },
   "outputs": [],
   "source": [
    "# ID data (CIFAR-100 test)\n",
    "_, _, id_test_loader = get_cifar100_loaders(\n",
    "    data_dir=config['data']['data_dir'],\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=config['data']['num_workers']\n",
    ")\n",
    "\n",
    "# OOD data (proportional sampling)\n",
    "ood_loaders = get_ood_loaders(\n",
    "    ood_datasets=config['ood']['datasets'],\n",
    "    data_dir=config['data']['data_dir'],\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    sampling_ratio=config['ood']['sampling_ratio']\n",
    ")\n",
    "\n",
    "print(f\"ID test samples: {len(id_test_loader.dataset)}\")\n",
    "\n",
    "print(\"OOD samples:\")\n",
    "for name, loader in ood_loaders.items():\n",
    "    print(f\"({name}) : {len(loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a117fc8",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Initialize Scorers"
   },
   "outputs": [],
   "source": [
    "def initialize_scorers(model):\n",
    "    return {\n",
    "        'MSP': MSPScorer(model, device),\n",
    "        'MaxLogit': MaxLogitScorer(model, device),\n",
    "        'Energy': EnergyScorer(model, device),\n",
    "        'Mahalanobis': MahalanobisScorer(model, device),\n",
    "        'ViM': ViMScorer(model, device),\n",
    "        'NECO': NECOScorer(model, device)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0e300",
   "metadata": {
    "title": "Main Evaluation Loop"
   },
   "outputs": [],
   "source": [
    "# Extract epoch numbers\n",
    "def get_epoch_num(path):\n",
    "    import re\n",
    "    match = re.search(r'epoch(\\d+)', path)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "checkpoint_dir = config['paths']['checkpoints']\n",
    "checkpoints = sorted(glob.glob(f\"{checkpoint_dir}/resnet18_cifar100_epoch*.pth\"), key = get_epoch_num)\n",
    "\n",
    "checkpoint_epochs = [get_epoch_num(cp) for cp in checkpoints]\n",
    "tpt_mask = config['ood']['tpt_mask']\n",
    "\n",
    "# Results storage\n",
    "results = {\n",
    "    'config': {\n",
    "        'epochs': checkpoint_epochs,\n",
    "        'ood_datasets': config['ood']['datasets'],\n",
    "        'sampling_ratio': config['ood']['sampling_ratio'],\n",
    "        'tpt_mask': tpt_mask\n",
    "    },\n",
    "    'scorers': {}\n",
    "}\n",
    "\n",
    "# init structure for each ood dataset\n",
    "for scorer_name in ['MSP', 'MaxLogit', 'Energy', 'Mahalanobis', 'ViM', 'NECO']:\n",
    "    results['scorers'][scorer_name] = {}\n",
    "    \n",
    "    for ood_dataset in config['ood']['datasets']:\n",
    "        results['scorers'][scorer_name][ood_dataset] = {\n",
    "            'auroc': [],\n",
    "            'fpr95': []\n",
    "        }\n",
    "\n",
    "print(f\"\\nEvaluating {len(checkpoints)} checkpoints...\")\n",
    "print(f\"TPT mask: {tpt_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76013539",
   "metadata": {
    "title": "Evaluation Loop"
   },
   "outputs": [],
   "source": [
    "for epoch_idx, (checkpoint_path, epoch) in enumerate(zip(checkpoints, checkpoint_epochs)):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Checkpoint: Epoch {epoch} ({epoch_idx+1}/{len(checkpoints)})\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load model\n",
    "    model = ResNet18(num_classes=config['model']['num_classes'])\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize scorers\n",
    "    scorers = initialize_scorers(model)\n",
    "    \n",
    "    # Fit statistics-based scorers (once per checkpoint)\n",
    "    print(\"\\nFitting statistics-based scorers...\")\n",
    "    train_loader, _, _ = get_cifar100_loaders(\n",
    "        data_dir=config['data']['data_dir'],\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        num_workers=config['data']['num_workers']\n",
    "    )\n",
    "    \n",
    "    for name in ['Mahalanobis', 'ViM', 'NECO']:\n",
    "        if name == 'NECO' and not tpt_mask[epoch_idx]:\n",
    "            continue  # Skip NECO if not in TPT\n",
    "        print(f\"  Fitting {name}...\")\n",
    "        scorers[name].fit(train_loader, num_classes=config['model']['num_classes'])\n",
    "    \n",
    "    # Evaluate each scorer\n",
    "    print(\"\\nEvaluating scorers...\")\n",
    "    \n",
    "    # Compute ID scores once (same for all OOD datasets)\n",
    "    print(\"  Computing ID scores...\")\n",
    "    id_scores_dict = {}\n",
    "    for scorer_name, scorer in scorers.items():\n",
    "        if scorer_name == 'NECO' and not tpt_mask[epoch_idx]:\n",
    "            continue\n",
    "        id_scores_dict[scorer_name] = scorer.score_loader(id_test_loader)\n",
    "    \n",
    "    # Evaluate against each OOD dataset separately\n",
    "    for dataset_name, ood_loader_single in ood_loaders.items():\n",
    "        print(f\"\\n  OOD Dataset: {dataset_name}\")\n",
    "        \n",
    "        for scorer_name, scorer in scorers.items():\n",
    "            # Skip NECO if not in TPT\n",
    "            if scorer_name == 'NECO' and not tpt_mask[epoch_idx]:\n",
    "                if dataset_name == list(ood_loaders.keys())[0]:  # Print only once\n",
    "                    print(f\"    {scorer_name}: Skipped (not in TPT)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"    {scorer_name}...\", end=' ')\n",
    "            \n",
    "            # Get ID scores (already computed)\n",
    "            id_scores = id_scores_dict[scorer_name]\n",
    "            \n",
    "            # Compute OOD scores for this dataset\n",
    "            ood_scores = scorer.score_loader(ood_loader_single)\n",
    "            \n",
    "            # Compute metrics\n",
    "            auroc = compute_auroc(id_scores, ood_scores)\n",
    "            fpr95 = compute_fpr_at_tpr(id_scores, ood_scores, tpr_target=0.95)\n",
    "            \n",
    "            # Store results per dataset\n",
    "            results['scorers'][scorer_name][dataset_name]['auroc'].append(auroc)\n",
    "            results['scorers'][scorer_name][dataset_name]['fpr95'].append(fpr95)\n",
    "            \n",
    "            print(f\"AUROC: {auroc:.3f}, FPR@95: {fpr95:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e191a",
   "metadata": {
    "title": "Save Results"
   },
   "outputs": [],
   "source": [
    "output_dir = config['paths']['ood_detection']\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save pickle\n",
    "results_path = os.path.join(output_dir, 'ood_scores_results.pkl')\n",
    "with open(results_path, 'wb') as f:  \n",
    "    pickle.dump(results, f)\n",
    "print(f\"\\nResults saved: {results_path}\")\n",
    "\n",
    "# Save CSV summary\n",
    "import pandas as pd\n",
    "summary_data = []\n",
    "\n",
    "for scorer_name, datasets_dict in results['scorers'].items():\n",
    "    for dataset_name, metrics in datasets_dict.items():\n",
    "        if len(metrics['auroc']) > 0: \n",
    "            summary_data.append({\n",
    "                'Scorer': scorer_name,\n",
    "                'OOD Dataset': dataset_name,\n",
    "                'Final AUROC': metrics['auroc'][-1],\n",
    "                'Final FPR@95': metrics['fpr95'][-1],\n",
    "                'Epochs Evaluated': len(metrics['auroc'])\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "csv_path = os.path.join(output_dir, 'ood_scores_summary.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nCSV summary saved: {csv_path}\")\n",
    "print(\"\\n\", df)\n",
    "\n",
    "# Also save aggregated summary (average across datasets)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Aggregated Results (Average across OOD datasets)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "agg_data = []\n",
    "for scorer_name, datasets_dict in results['scorers'].items():\n",
    "    aurocs = []\n",
    "    fprs = []\n",
    "    for dataset_name, metrics in datasets_dict.items():\n",
    "        if len(metrics['auroc']) > 0:\n",
    "            aurocs.append(metrics['auroc'][-1])\n",
    "            fprs.append(metrics['fpr95'][-1])\n",
    "    \n",
    "    if aurocs:\n",
    "        agg_data.append({\n",
    "            'Scorer': scorer_name,\n",
    "            'Avg AUROC': np.mean(aurocs),\n",
    "            'Avg FPR@95': np.mean(fprs),\n",
    "            'Std AUROC': np.std(aurocs),\n",
    "            'Std FPR@95': np.std(fprs)\n",
    "        })\n",
    "\n",
    "df_agg = pd.DataFrame(agg_data)\n",
    "agg_path = os.path.join(output_dir, 'ood_scores_aggregated.csv')\n",
    "df_agg.to_csv(agg_path, index=False)\n",
    "print(df_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe5483",
   "metadata": {
    "title": "Plot Results"
   },
   "outputs": [],
   "source": [
    "plot_ood_scores_per_dataset(results, save_dir=output_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OOD Detection Evaluation Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
