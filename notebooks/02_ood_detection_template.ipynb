{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15beea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OOD Detection Notebook Template\n",
    "Convert to .ipynb using: jupytext --to notebook 02_ood_detection.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bf9d1",
   "metadata": {},
   "source": [
    "# Q2: Out-of-Distribution Detection\n",
    "\n",
    "Evaluate 6 OOD scoring methods across training epochs:\n",
    "- Output-based: MSP, MaxLogit, Energy\n",
    "- Distance-based: Mahalanobis  \n",
    "- Feature-based: ViM, NECO (TPT only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d3a87",
   "metadata": {
    "title": "Setup"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/OOD-Detection-Project---CSC_5IA23')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models.resnet import ResNet18\n",
    "from src.data.datasets import get_cifar100_loaders, get_ood_loaders\n",
    "from src.ood_scores import (\n",
    "    MSPScorer, MaxLogitScorer, EnergyScorer,\n",
    "    MahalanobisScorer, ViMScorer, NECOScorer\n",
    ")\n",
    "from src.utils.ood_metrics import compute_auroc, compute_fpr_at_tpr\n",
    "from src.utils.visualization import plot_ood_scores\n",
    "\n",
    "# Load config\n",
    "with open('configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0404ee8",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Load Data"
   },
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "# ID data (CIFAR-100 test)\n",
    "_, _, id_test_loader = get_cifar100_loaders(\n",
    "    data_dir=config['data']['data_dir'],\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=config['data']['num_workers']\n",
    ")\n",
    "\n",
    "# OOD data (proportional sampling)\n",
    "ood_loader = get_ood_loaders(\n",
    "    ood_datasets=config['ood']['datasets'],\n",
    "    data_dir=config['data']['data_dir'],\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    sampling_ratio=config['ood']['sampling_ratio']\n",
    ")\n",
    "\n",
    "print(f\"ID test samples: {len(id_test_loader.dataset)}\")\n",
    "print(f\"OOD samples: {len(ood_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a117fc8",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Initialize Scorers"
   },
   "outputs": [],
   "source": [
    "def initialize_scorers(model):\n",
    "    return {\n",
    "        'MSP': MSPScorer(model, device),\n",
    "        'MaxLogit': MaxLogitScorer(model, device),\n",
    "        'Energy': EnergyScorer(model, device),\n",
    "        'Mahalanobis': MahalanobisScorer(model, device),\n",
    "        'ViM': ViMScorer(model, device),\n",
    "        'NECO': NECOScorer(model, device)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0e300",
   "metadata": {
    "title": "Main Evaluation Loop"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = config['paths']['checkpoints']\n",
    "checkpoints = sorted(glob.glob(f\"{checkpoint_dir}/resnet18_cifar100_epoch*.pth\"))\n",
    "\n",
    "# Extract epoch numbers\n",
    "def get_epoch_num(path):\n",
    "    import re\n",
    "    match = re.search(r'epoch(\\d+)', path)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "checkpoint_epochs = [get_epoch_num(cp) for cp in checkpoints]\n",
    "tpt_mask = config['ood']['tpt_mask']\n",
    "\n",
    "# Results storage\n",
    "results = {\n",
    "    'config': {\n",
    "        'epochs': checkpoint_epochs,\n",
    "        'ood_datasets': config['ood']['datasets'],\n",
    "        'sampling_ratio': config['ood']['sampling_ratio'],\n",
    "        'tpt_mask': tpt_mask\n",
    "    },\n",
    "    'scorers': {\n",
    "        'MSP': {'auroc': [], 'fpr95': []},\n",
    "        'MaxLogit': {'auroc': [], 'fpr95': []},\n",
    "        'Energy': {'auroc': [], 'fpr95': []},\n",
    "        'Mahalanobis': {'auroc': [], 'fpr95': []},\n",
    "        'ViM': {'auroc': [], 'fpr95': []},\n",
    "        'NECO': {'auroc': [], 'fpr95': []}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nEvaluating {len(checkpoints)} checkpoints...\")\n",
    "print(f\"TPT mask: {tpt_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76013539",
   "metadata": {
    "title": "Evaluation Loop"
   },
   "outputs": [],
   "source": [
    "for epoch_idx, (checkpoint_path, epoch) in enumerate(zip(checkpoints, checkpoint_epochs)):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Checkpoint: Epoch {epoch} ({epoch_idx+1}/{len(checkpoints)})\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load model\n",
    "    model = ResNet18(num_classes=config['model']['num_classes'])\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize scorers\n",
    "    scorers = initialize_scorers(model)\n",
    "    \n",
    "    # Fit statistics-based scorers (once per checkpoint)\n",
    "    print(\"\\nFitting statistics-based scorers...\")\n",
    "    train_loader, _, _ = get_cifar100_loaders(\n",
    "        data_dir=config['data']['data_dir'],\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        num_workers=config['data']['num_workers']\n",
    "    )\n",
    "    \n",
    "    for name in ['Mahalanobis', 'ViM', 'NECO']:\n",
    "        if name == 'NECO' and not tpt_mask[epoch_idx]:\n",
    "            continue  # Skip NECO if not in TPT\n",
    "        print(f\"  Fitting {name}...\")\n",
    "        scorers[name].fit(train_loader, num_classes=config['model']['num_classes'])\n",
    "    \n",
    "    # Evaluate each scorer\n",
    "    print(\"\\nEvaluating scorers...\")\n",
    "    for scorer_name, scorer in scorers.items():\n",
    "        # Skip NECO if not in TPT\n",
    "        if scorer_name == 'NECO' and not tpt_mask[epoch_idx]:\n",
    "            print(f\"  {scorer_name}: Skipped (not in TPT)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  {scorer_name}...\", end=' ')\n",
    "        \n",
    "        # Compute ID scores\n",
    "        id_scores = scorer.score_loader(id_test_loader)\n",
    "        \n",
    "        # Compute OOD scores\n",
    "        ood_scores = scorer.score_loader(ood_loader)\n",
    "        \n",
    "        # Compute metrics\n",
    "        auroc = compute_auroc(id_scores, ood_scores)\n",
    "        fpr95 = compute_fpr_at_tpr(id_scores, ood_scores, tpr_target=0.95)\n",
    "        \n",
    "        # Store results\n",
    "        results['scorers'][scorer_name]['auroc'].append(auroc)\n",
    "        results['scorers'][scorer_name]['fpr95'].append(fpr95)\n",
    "        \n",
    "        print(f\"AUROC: {auroc:.3f}, FPR@95: {fpr95:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e191a",
   "metadata": {
    "title": "Save Results"
   },
   "outputs": [],
   "source": [
    "output_dir = config['paths']['ood_detection']\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save pickle\n",
    "results_path = os.path.join(output_dir, 'ood_scores_results.pkl')\n",
    "with open(results_path, '() as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"\\nResults saved: {results_path}\")\n",
    "\n",
    "# Save CSV summary\n",
    "import pandas as pd\n",
    "summary_data = []\n",
    "for scorer_name, data in results['scorers'].items():\n",
    "    if len(data['auroc']) > 0:\n",
    "        summary_data.append({\n",
    "            'Scorer': scorer_name,\n",
    "            'Final AUROC': data['auroc'][-1],\n",
    "            'Final FPR@95': data['fpr95'][-1],\n",
    "            'Epochs Evaluated': len(data['auroc'])\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "csv_path = os.path.join(output_dir, 'ood_scores_summary.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV summary saved: {csv_path}\")\n",
    "print(\"\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe5483",
   "metadata": {
    "title": "Plot Results"
   },
   "outputs": [],
   "source": [
    "plot_ood_scores(results, save_dir=output_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OOD Detection Evaluation Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
