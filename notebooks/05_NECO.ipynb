{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: NECO â€” OOD Detection via Neural Collapse\n",
    "\n",
    "This notebook implements the **NECO score** for OOD detection and benchmarks it against baselines.\n",
    "\n",
    "The NECO score (eq. 6 of the paper) exploits NC1 + NC2 + NC5 to separate ID from OOD:\n",
    "\n",
    "$$\\text{NECO}(\\mathbf{x}) = \\frac{\\|P \\, h_\\omega(\\mathbf{x})\\|}{\\|h_\\omega(\\mathbf{x})\\|}$$\n",
    "\n",
    "where $P$ projects onto the top-$d$ principal components of ID features.\n",
    "\n",
    "We study:\n",
    "1. **NECO score** computation on SVHN and CIFAR-10 as OOD datasets\n",
    "2. **PCA dimension sweep** â€” finding the optimal ETF approximation\n",
    "3. **Score distributions** â€” histograms & ROC curves\n",
    "4. **Comparison with baselines** â€” MSP, MaxLogit, Energy\n",
    "5. **PCA 2D visualization** â€” how ID clusters separate from OOD\n",
    "\n",
    "Reference:\n",
    "> Ben Ammar et al., *\"NECO: Neural Collapse Based Out-of-Distribution Detection\"*, ICLR 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os\n",
    "if not os.path.exists('/content/OOD-Detection-Project---CSC_5IA23'):\n",
    "    !git clone https://github.com/DiegoFleury/OOD-Detection-Project---CSC_5IA23/\n",
    "\n",
    "!git stash\n",
    "!git checkout contente\n",
    "!git pull\n",
    "%cd /content/OOD-Detection-Project---CSC_5IA23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision matplotlib seaborn scikit-learn pyyaml imageio tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "from src.models import ResNet18\n",
    "from src.data import get_cifar100_loaders\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID data: CIFAR-100\n",
    "print(\"Loading CIFAR-100 (ID) dataset...\")\n",
    "\n",
    "train_loader, val_loader, test_loader = get_cifar100_loaders(\n",
    "    data_dir=config['data']['data_dir'],\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    augment=False,\n",
    "    val_split=config['training']['val_split']\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD data: SVHN + CIFAR-10\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "ood_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# SVHN\n",
    "print(\"Loading SVHN (OOD)...\")\n",
    "svhn_dataset = torchvision.datasets.SVHN(\n",
    "    root=config['data']['data_dir'], split='test',\n",
    "    transform=ood_transform, download=True,\n",
    ")\n",
    "svhn_loader = torch.utils.data.DataLoader(\n",
    "    svhn_dataset, batch_size=config['training']['batch_size'],\n",
    "    shuffle=False, num_workers=config['data']['num_workers'],\n",
    ")\n",
    "print(f\"SVHN test samples: {len(svhn_dataset)}\")\n",
    "\n",
    "# CIFAR-10\n",
    "print(\"Loading CIFAR-10 (OOD)...\")\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=config['data']['data_dir'], train=False,\n",
    "    transform=ood_transform, download=True,\n",
    ")\n",
    "cifar10_loader = torch.utils.data.DataLoader(\n",
    "    cifar10_dataset, batch_size=config['training']['batch_size'],\n",
    "    shuffle=False, num_workers=config['data']['num_workers'],\n",
    ")\n",
    "print(f\"CIFAR-10 test samples: {len(cifar10_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating ResNet-18 model...\")\n",
    "model = ResNet18(num_classes=config['model']['num_classes'])\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint_dir = config['paths']['checkpoints']\n",
    "checkpoints = glob.glob(os.path.join(checkpoint_dir, 'resnet18_cifar100_*.pth'))\n",
    "\n",
    "def get_epoch_num(path):\n",
    "    match = re.search(r'epoch(\\d+)', path)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "latest = max(checkpoints, key=get_epoch_num)\n",
    "epoch_num = get_epoch_num(latest)\n",
    "\n",
    "ckpt = torch.load(latest, map_location=device, weights_only=False)\n",
    "if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "elif isinstance(ckpt, dict) and 'state_dict' in ckpt:\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "else:\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Loaded: {os.path.basename(latest)} (epoch {epoch_num})\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import NECO Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neural_collapse.neco import (\n",
    "    compute_neco_scores,\n",
    "    compute_baseline_scores,\n",
    "    evaluate_ood_detection,\n",
    "    plot_neco_distributions,\n",
    "    plot_neco_pca_2d,\n",
    "    plot_pca_dim_sweep,\n",
    "    NECOResult,\n",
    ")\n",
    "\n",
    "print(\"âœ… NECO module imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directories\n",
    "figures_dir = os.path.join(config['paths']['figures'], 'neco')\n",
    "metrics_dir = config['paths']['metrics']\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NECO Score â€” OOD Detection\n",
    "\n",
    "The NECO score projects features onto the top-$(C-1)$ principal components fitted on ID training data.\n",
    "\n",
    "**Intuition:** If NC1+NC2 hold, ID features live in a $(C-1)$-dimensional ETF subspace. NC5 pushes OOD features orthogonal to this subspace, so their projection norm is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECO score â€” SVHN as OOD\n",
    "print(\"ðŸ”¬ Computing NECO scores (OOD = SVHN)...\")\n",
    "\n",
    "result_svhn = compute_neco_scores(\n",
    "    model=model,\n",
    "    id_loader=test_loader,\n",
    "    ood_loader=svhn_loader,\n",
    "    device=device,\n",
    "    num_classes=config['model']['num_classes'],\n",
    "    id_train_loader=train_loader,\n",
    "    pca_dim=config['model']['num_classes'] - 1,   # C-1 = 99\n",
    "    use_maxlogit=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š CIFAR-100 vs SVHN:\")\n",
    "print(f\"   PCA dim:  {result_svhn.pca_dim}\")\n",
    "print(f\"   AUROC:    {result_svhn.auroc:.4f} ({result_svhn.auroc*100:.2f}%)\")\n",
    "print(f\"   FPR95:    {result_svhn.fpr95:.4f} ({result_svhn.fpr95*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECO score â€” CIFAR-10 as OOD\n",
    "print(\"ðŸ”¬ Computing NECO scores (OOD = CIFAR-10)...\")\n",
    "\n",
    "result_cifar10 = compute_neco_scores(\n",
    "    model=model,\n",
    "    id_loader=test_loader,\n",
    "    ood_loader=cifar10_loader,\n",
    "    device=device,\n",
    "    num_classes=config['model']['num_classes'],\n",
    "    id_train_loader=train_loader,\n",
    "    pca_dim=config['model']['num_classes'] - 1,\n",
    "    use_maxlogit=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š CIFAR-100 vs CIFAR-10:\")\n",
    "print(f\"   PCA dim:  {result_cifar10.pca_dim}\")\n",
    "print(f\"   AUROC:    {result_cifar10.auroc:.4f} ({result_cifar10.auroc*100:.2f}%)\")\n",
    "print(f\"   FPR95:    {result_cifar10.fpr95:.4f} ({result_cifar10.fpr95*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 NECO Score Distributions\n",
    "\n",
    "Reproducing Figures E.16 and E.17 from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dist_svhn = plot_neco_distributions(\n",
    "    result_svhn, id_name=\"CIFAR-100\", ood_name=\"SVHN\",\n",
    "    save_dir=figures_dir,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dist_c10 = plot_neco_distributions(\n",
    "    result_cifar10, id_name=\"CIFAR-100\", ood_name=\"CIFAR-10\",\n",
    "    save_dir=figures_dir,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA Dimension Sweep\n",
    "\n",
    "Following Section C.5 and Table C.5: we vary the PCA dimension $d$ to find the optimal ETF subspace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep PCA dimensions\n",
    "pca_dims = [10, 20, 30, 50, 70, 99, 130, 150, 200, 250, 300, 400, 500]\n",
    "pca_dims = [d for d in pca_dims if d < 510]  # ResNet-18 feature dim = 512\n",
    "\n",
    "sweep_svhn = {}\n",
    "sweep_cifar10 = {}\n",
    "\n",
    "print(\"Sweeping PCA dimensions...\")\n",
    "for d in pca_dims:\n",
    "    sweep_svhn[d] = compute_neco_scores(\n",
    "        model=model, id_loader=test_loader, ood_loader=svhn_loader,\n",
    "        device=device, num_classes=config['model']['num_classes'],\n",
    "        id_train_loader=train_loader, pca_dim=d,\n",
    "    )\n",
    "    sweep_cifar10[d] = compute_neco_scores(\n",
    "        model=model, id_loader=test_loader, ood_loader=cifar10_loader,\n",
    "        device=device, num_classes=config['model']['num_classes'],\n",
    "        id_train_loader=train_loader, pca_dim=d,\n",
    "    )\n",
    "\n",
    "    print(f\"  d={d:>3d} | SVHN: AUROC={sweep_svhn[d].auroc:.4f} \"\n",
    "          f\"FPR95={sweep_svhn[d].fpr95:.4f} | \"\n",
    "          f\"CIFAR-10: AUROC={sweep_cifar10[d].auroc:.4f} \"\n",
    "          f\"FPR95={sweep_cifar10[d].fpr95:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sweep â€” SVHN\n",
    "fig_sweep_svhn = plot_pca_dim_sweep(\n",
    "    sweep_svhn, ood_name=\"SVHN\", save_dir=figures_dir,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Plot sweep â€” CIFAR-10\n",
    "fig_sweep_c10 = plot_pca_dim_sweep(\n",
    "    sweep_cifar10, ood_name=\"CIFAR-10\", save_dir=figures_dir,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Best dimensions\n",
    "for name, sweep in [('SVHN', sweep_svhn), ('CIFAR-10', sweep_cifar10)]:\n",
    "    best_d = min(sweep.keys(), key=lambda d: sweep[d].fpr95)\n",
    "    print(f\"Best d for {name}: {best_d} \"\n",
    "          f\"(AUROC={sweep[best_d].auroc:.4f}, FPR95={sweep[best_d].fpr95:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined PCA sweep plot (both OOD datasets)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "dims = sorted(sweep_svhn.keys())\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(dims, [sweep_svhn[d].auroc * 100 for d in dims],\n",
    "        'b-o', markersize=4, label='SVHN')\n",
    "ax.plot(dims, [sweep_cifar10[d].auroc * 100 for d in dims],\n",
    "        'g-s', markersize=4, label='CIFAR-10')\n",
    "ax.set_xlabel('PCA dimension (d)', fontsize=12)\n",
    "ax.set_ylabel('AUROC (%)', fontsize=12)\n",
    "ax.set_title('AUROC vs PCA Dimension', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(dims, [sweep_svhn[d].fpr95 * 100 for d in dims],\n",
    "        'b-o', markersize=4, label='SVHN')\n",
    "ax.plot(dims, [sweep_cifar10[d].fpr95 * 100 for d in dims],\n",
    "        'g-s', markersize=4, label='CIFAR-10')\n",
    "ax.set_xlabel('PCA dimension (d)', fontsize=12)\n",
    "ax.set_ylabel('FPR95 (%)', fontsize=12)\n",
    "ax.set_title('FPR95 vs PCA Dimension', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('NECO: Effect of PCA Dimension (ResNet-18 / CIFAR-100)', fontsize=15, y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join(figures_dir, 'neco_pca_dim_sweep_combined.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA 2D Feature Projections\n",
    "\n",
    "Reproducing Figure 2 / D.14: ID classes form distinct clusters on the ETF subspace, while OOD data projects near the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pca_svhn = plot_neco_pca_2d(\n",
    "    model=model, id_loader=test_loader, ood_loader=svhn_loader,\n",
    "    device=device, num_classes=config['model']['num_classes'],\n",
    "    id_name=\"CIFAR-100\", ood_name=\"SVHN\",\n",
    "    max_samples=3000, save_dir=figures_dir,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pca_c10 = plot_neco_pca_2d(\n",
    "    model=model, id_loader=test_loader, ood_loader=cifar10_loader,\n",
    "    device=device, num_classes=config['model']['num_classes'],\n",
    "    id_name=\"CIFAR-100\", ood_name=\"CIFAR-10\",\n",
    "    max_samples=3000, save_dir=figures_dir,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Baselines\n",
    "\n",
    "We compare NECO against:\n",
    "- **MSP** â€” Maximum Softmax Probability (Hendrycks & Gimpel, 2017)\n",
    "- **MaxLogit** â€” Max Logit (Hendrycks et al., 2022)\n",
    "- **Energy** â€” Energy score = logsumexp(logits) (Liu et al., 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baselines\n",
    "baselines = {}\n",
    "\n",
    "for method in ['msp', 'maxlogit', 'energy']:\n",
    "    for ood_name, ood_ldr in [('SVHN', svhn_loader), ('CIFAR-10', cifar10_loader)]:\n",
    "        id_scores = compute_baseline_scores(model, test_loader, device, method)\n",
    "        ood_scores = compute_baseline_scores(model, ood_ldr, device, method)\n",
    "        metrics = evaluate_ood_detection(id_scores, ood_scores)\n",
    "        baselines[(method, ood_name)] = metrics\n",
    "        print(f\"{method:>10s} | {ood_name:>10s} | \"\n",
    "              f\"AUROC={metrics['auroc']:.4f} FPR95={metrics['fpr95']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OOD DETECTION RESULTS â€” ResNet-18 / CIFAR-100\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':>12s} | {'OOD Dataset':>10s} | {'AUROC (%)':>10s} | {'FPR95 (%)':>10s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "method_labels = {'msp': 'Softmax', 'maxlogit': 'MaxLogit', 'energy': 'Energy'}\n",
    "\n",
    "for method in ['msp', 'maxlogit', 'energy']:\n",
    "    for ood_name in ['SVHN', 'CIFAR-10']:\n",
    "        m = baselines[(method, ood_name)]\n",
    "        print(f\"{method_labels[method]:>12s} | {ood_name:>10s} | \"\n",
    "              f\"{m['auroc']*100:>10.2f} | {m['fpr95']*100:>10.2f}\")\n",
    "\n",
    "print(f\"{'NECO':>12s} | {'SVHN':>10s} | \"\n",
    "      f\"{result_svhn.auroc*100:>10.2f} | {result_svhn.fpr95*100:>10.2f}\")\n",
    "print(f\"{'NECO':>12s} | {'CIFAR-10':>10s} | \"\n",
    "      f\"{result_cifar10.auroc*100:>10.2f} | {result_cifar10.fpr95*100:>10.2f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Averages\n",
    "print(\"\\nAverages across OOD datasets:\")\n",
    "for method in ['msp', 'maxlogit', 'energy']:\n",
    "    avg_auroc = np.mean([baselines[(method, o)]['auroc'] for o in ['SVHN', 'CIFAR-10']])\n",
    "    avg_fpr = np.mean([baselines[(method, o)]['fpr95'] for o in ['SVHN', 'CIFAR-10']])\n",
    "    print(f\"  {method_labels[method]:>10s}: AUROC={avg_auroc*100:.2f}%  FPR95={avg_fpr*100:.2f}%\")\n",
    "\n",
    "neco_avg_auroc = np.mean([result_svhn.auroc, result_cifar10.auroc])\n",
    "neco_avg_fpr = np.mean([result_svhn.fpr95, result_cifar10.fpr95])\n",
    "print(f\"  {'NECO':>10s}: AUROC={neco_avg_auroc*100:.2f}%  FPR95={neco_avg_fpr*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "neco_results = {\n",
    "    'model': 'ResNet-18',\n",
    "    'id_dataset': 'CIFAR-100',\n",
    "    'checkpoint_epoch': epoch_num,\n",
    "    'neco': {\n",
    "        'SVHN': {\n",
    "            'auroc': result_svhn.auroc,\n",
    "            'fpr95': result_svhn.fpr95,\n",
    "            'pca_dim': result_svhn.pca_dim,\n",
    "        },\n",
    "        'CIFAR-10': {\n",
    "            'auroc': result_cifar10.auroc,\n",
    "            'fpr95': result_cifar10.fpr95,\n",
    "            'pca_dim': result_cifar10.pca_dim,\n",
    "        },\n",
    "    },\n",
    "    'baselines': {\n",
    "        method: {\n",
    "            ood: baselines[(method, ood)]\n",
    "            for ood in ['SVHN', 'CIFAR-10']\n",
    "        }\n",
    "        for method in ['msp', 'maxlogit', 'energy']\n",
    "    },\n",
    "    'pca_sweep': {\n",
    "        'SVHN': {d: {'auroc': r.auroc, 'fpr95': r.fpr95} for d, r in sweep_svhn.items()},\n",
    "        'CIFAR-10': {d: {'auroc': r.auroc, 'fpr95': r.fpr95} for d, r in sweep_cifar10.items()},\n",
    "    },\n",
    "}\n",
    "\n",
    "json_path = os.path.join(metrics_dir, 'neco_results.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(neco_results, f, indent=2)\n",
    "print(f\"ðŸ’¾ Saved: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NECO OOD DETECTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel: ResNet-18 / CIFAR-100 (epoch {epoch_num})\")\n",
    "print(f\"PCA dim: {result_svhn.pca_dim} (C-1 = {config['model']['num_classes']-1})\")\n",
    "\n",
    "print(f\"\\n--- NECO Results ---\")\n",
    "print(f\"  SVHN:     AUROC = {result_svhn.auroc*100:.2f}%  |  FPR95 = {result_svhn.fpr95*100:.2f}%\")\n",
    "print(f\"  CIFAR-10: AUROC = {result_cifar10.auroc*100:.2f}%  |  FPR95 = {result_cifar10.fpr95*100:.2f}%\")\n",
    "print(f\"  Average:  AUROC = {neco_avg_auroc*100:.2f}%  |  FPR95 = {neco_avg_fpr*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n--- Best baseline ---\")\n",
    "best_method = min(['msp', 'maxlogit', 'energy'],\n",
    "    key=lambda m: np.mean([baselines[(m, o)]['fpr95'] for o in ['SVHN', 'CIFAR-10']]))\n",
    "best_avg_fpr = np.mean([baselines[(best_method, o)]['fpr95'] for o in ['SVHN', 'CIFAR-10']])\n",
    "best_avg_auroc = np.mean([baselines[(best_method, o)]['auroc'] for o in ['SVHN', 'CIFAR-10']])\n",
    "print(f\"  {method_labels[best_method]}: AUROC={best_avg_auroc*100:.2f}% FPR95={best_avg_fpr*100:.2f}%\")\n",
    "\n",
    "if neco_avg_fpr < best_avg_fpr:\n",
    "    improvement = (best_avg_fpr - neco_avg_fpr) / best_avg_fpr * 100\n",
    "    print(f\"  â†’ NECO improves FPR95 by {improvement:.1f}% relative\")\n",
    "\n",
    "print(f\"\\n--- Files Saved ---\")\n",
    "print(f\"  Figures:  {figures_dir}/\")\n",
    "print(f\"  Metrics:  {json_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Commit Results to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git add results/figures/neco/\n",
    "# !git add results/metrics/neco_results.json\n",
    "# !git commit -m \"Add Q5 NECO OOD detection: scores, baselines, PCA sweep\"\n",
    "# !git push\n",
    "#\n",
    "# print(\"Results committed to GitHub!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
